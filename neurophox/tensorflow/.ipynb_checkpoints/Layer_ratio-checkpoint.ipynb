{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81526e98",
   "metadata": {},
   "source": [
    "from typing import List, Tuple, Optional, Callable\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Activation\n",
    "import numpy as np\n",
    "\n",
    "from ..numpy.generic import MeshPhases\n",
    "from ..meshmodel import MeshModel\n",
    "from ..helpers import pairwise_off_diag_permutation, plot_complex_matrix, inverse_permutation\n",
    "from ..config import TF_COMPLEX, BLOCH, SINGLEMODE\n",
    "\n",
    "\n",
    "class TransformerLayer(Layer):\n",
    "    \"\"\"Base transformer class for transformer layers (invertible functions, usually linear)\n",
    "    Args:\n",
    "        units: Dimension of the input to be transformed by the transformer\n",
    "        activation: Nonlinear activation function (:code:`None` if there's no nonlinearity)\n",
    "    \"\"\"\n",
    "    def __init__(self, units: int, activation: Activation = None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        super(TransformerLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def transform(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Transform inputs using layer (needs to be overwritten by child classes)\n",
    "        Args:\n",
    "            inputs: Inputs to be transformed by layer\n",
    "        Returns:\n",
    "            Transformed inputs\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Needs to be overwritten by child class.\")\n",
    "\n",
    "    def inverse_transform(self, outputs: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Transform outputs using layer\n",
    "        Args:\n",
    "            outputs: Outputs to be inverse-transformed by layer\n",
    "        Returns:\n",
    "            Transformed outputs\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Needs to be overwritten by child class.\")\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        outputs = self.transform(inputs)\n",
    "        if self.activation:\n",
    "            outputs = self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "    @property\n",
    "    def matrix(self):\n",
    "        \"\"\"\n",
    "        Shortcut of :code:`transformer.transform(np.eye(self.units))`\n",
    "        Returns:\n",
    "            Matrix implemented by layer\n",
    "        \"\"\"\n",
    "        identity_matrix = np.eye(self.units, dtype=np.complex64)\n",
    "        return self.transform(identity_matrix).numpy()\n",
    "\n",
    "    @property\n",
    "    def inverse_matrix(self):\n",
    "        \"\"\"\n",
    "        Shortcut of :code:`transformer.inverse_transform(np.eye(self.units))`\n",
    "        Returns:\n",
    "            Inverse matrix implemented by layer\n",
    "        \"\"\"\n",
    "        identity_matrix = np.eye(self.units, dtype=np.complex64)\n",
    "        return self.inverse_transform(identity_matrix).numpy()\n",
    "\n",
    "    def plot(self, plt):\n",
    "        \"\"\"\n",
    "        Plot :code:`transformer.matrix`.\n",
    "        Args:\n",
    "            plt: :code:`matplotlib.pyplot` for plotting\n",
    "        \"\"\"\n",
    "        plot_complex_matrix(plt, self.matrix)\n",
    "\n",
    "\n",
    "class CompoundTransformerLayer(TransformerLayer):\n",
    "    \"\"\"Compound transformer class for unitary matrices\n",
    "    Args:\n",
    "        units: Dimension of the input to be transformed by the transformer\n",
    "        transformer_list: List of :class:`Transformer` objects to apply to the inputs\n",
    "        is_complex: Whether the input to be transformed is complex\n",
    "    \"\"\"\n",
    "    def __init__(self, units: int, transformer_list: List[TransformerLayer]):\n",
    "        self.transformer_list = transformer_list\n",
    "        super(CompoundTransformerLayer, self).__init__(units=units)\n",
    "\n",
    "    def transform(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Inputs are transformed by :math:`L` transformer layers\n",
    "        :math:`T^{(\\ell)} \\in \\mathbb{C}^{N \\\\times N}` as follows:\n",
    "        .. math::\n",
    "            V_{\\mathrm{out}} = V_{\\mathrm{in}} \\prod_{\\ell=1}^L T_\\ell,\n",
    "        where :math:`V_{\\mathrm{out}}, V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`.\n",
    "        Args:\n",
    "            inputs: Input batch represented by the matrix :math:`V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`\n",
    "        Returns:\n",
    "            Transformed :code:`inputs`, :math:`V_{\\mathrm{out}}`\n",
    "        \"\"\"\n",
    "        outputs = inputs\n",
    "        for transformer in self.transformer_list:\n",
    "            outputs = transformer.transform(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def inverse_transform(self, outputs: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Outputs are inverse-transformed by :math:`L` transformer layers :math:`T^{(\\ell)} \\in \\mathbb{C}^{N \\\\times N}` as follows:\n",
    "        .. math::\n",
    "            V_{\\mathrm{in}} = V_{\\mathrm{out}} \\prod_{\\ell=L}^1 T_\\ell^{-1},\n",
    "        where :math:`V_{\\mathrm{out}}, V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`.\n",
    "        Args:\n",
    "            outputs: Output batch represented by the matrix :math:`V_{\\mathrm{out}} \\in \\mathbb{C}^{M \\\\times N}`\n",
    "        Returns:\n",
    "            Transformed :code:`outputs`, :math:`V_{\\mathrm{in}}`\n",
    "        \"\"\"\n",
    "        inputs = outputs\n",
    "        for transformer in self.transformer_list[::-1]:\n",
    "            inputs = transformer.inverse_transform(inputs)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class PermutationLayer(TransformerLayer):\n",
    "    \"\"\"Permutation layer\n",
    "    Args:\n",
    "        permuted_indices: order of indices for the permutation matrix (efficient permutation representation)\n",
    "    \"\"\"\n",
    "    def __init__(self, permuted_indices: np.ndarray):\n",
    "        super(PermutationLayer, self).__init__(units=permuted_indices.shape[0])\n",
    "        self.permuted_indices = np.asarray(permuted_indices, dtype=np.int32)\n",
    "        self.inv_permuted_indices = inverse_permutation(self.permuted_indices)\n",
    "\n",
    "    def transform(self, inputs: tf.Tensor):\n",
    "        \"\"\"\n",
    "        Performs the permutation for this layer represented by :math:`P` defined by `permuted_indices`:\n",
    "        .. math::\n",
    "            V_{\\mathrm{out}} = V_{\\mathrm{in}} P,\n",
    "        where :math:`P` is any :math:`N`-dimensional permutation and\n",
    "        :math:`V_{\\mathrm{out}}, V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`.\n",
    "        Args:\n",
    "            inputs: Input batch represented by the matrix :math:`V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`\n",
    "        Returns:\n",
    "            Permuted :code:`inputs`, :math:`V_{\\mathrm{out}}`\n",
    "        \"\"\"\n",
    "        return tf.gather(inputs, self.permuted_indices, axis=-1)\n",
    "\n",
    "    def inverse_transform(self, outputs: tf.Tensor):\n",
    "        \"\"\"\n",
    "        Performs the inverse permutation for this layer represented by :math:`P^{-1}` defined by `inv_permuted_indices`:\n",
    "        .. math::\n",
    "            V_{\\mathrm{in}} = V_{\\mathrm{out}} P^{-1},\n",
    "        where :math:`P` is any :math:`N`-dimensional permutation and\n",
    "        :math:`V_{\\mathrm{out}}, V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`.\n",
    "        Args:\n",
    "            outputs: :code:`outputs` batch represented by the matrix :math:`V_{\\mathrm{out}} \\in \\mathbb{C}^{M \\\\times N}`\n",
    "        Returns:\n",
    "            Permuted :code:`outputs`, :math:`V_{\\mathrm{in}}`\n",
    "        \"\"\"\n",
    "        return tf.gather(outputs, self.inv_permuted_indices, axis=-1)\n",
    "\n",
    "\n",
    "class MeshVerticalLayer(TransformerLayer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        diag: the diagonal terms to multiply\n",
    "        off_diag: the off-diagonal terms to multiply\n",
    "        left_perm: the permutation for the mesh vertical layer (prior to the coupling operation)\n",
    "        right_perm: the right permutation for the mesh vertical layer\n",
    "            (usually for the final layer and after the coupling operation)\n",
    "    \"\"\"\n",
    "    def __init__(self, pairwise_perm_idx: np.ndarray, diag: tf.Tensor, off_diag: tf.Tensor,\n",
    "                 right_perm: PermutationLayer = None, left_perm: PermutationLayer = None):\n",
    "        self.diag = diag\n",
    "        self.off_diag = off_diag\n",
    "        self.left_perm = left_perm\n",
    "        self.right_perm = right_perm\n",
    "        self.pairwise_perm_idx = pairwise_perm_idx\n",
    "        super(MeshVerticalLayer, self).__init__(pairwise_perm_idx.shape[0])\n",
    "\n",
    "    def transform(self, inputs: tf.Tensor):\n",
    "        \"\"\"\n",
    "        Propagate :code:`inputs` through single layer :math:`\\ell < L`\n",
    "        (where :math:`U_\\ell` represents the matrix for layer :math:`\\ell`):\n",
    "        .. math::\n",
    "            V_{\\mathrm{out}} = V_{\\mathrm{in}} U^{(\\ell')},\n",
    "        Args:\n",
    "            inputs: :code:`inputs` batch represented by the matrix :math:`V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`\n",
    "        Returns:\n",
    "            Propaged :code:`inputs` through single layer :math:`\\ell` to form an array\n",
    "            :math:`V_{\\mathrm{out}} \\in \\mathbb{C}^{M \\\\times N}`.\n",
    "        \"\"\"\n",
    "        outputs = inputs if self.left_perm is None else self.left_perm.transform(inputs)\n",
    "        outputs = outputs * self.diag + tf.gather(outputs * self.off_diag, self.pairwise_perm_idx, axis=-1)\n",
    "        return outputs if self.right_perm is None else self.right_perm.transform(outputs)\n",
    "\n",
    "    def inverse_transform(self, outputs: tf.Tensor):\n",
    "        \"\"\"\n",
    "        Inverse-propagate :code:`inputs` through single layer :math:`\\ell < L`\n",
    "        (where :math:`U_\\ell` represents the matrix for layer :math:`\\ell`):\n",
    "        .. math::\n",
    "            V_{\\mathrm{in}} = V_{\\mathrm{out}} (U^{(\\ell')})^\\dagger,\n",
    "        Args:\n",
    "            outputs: :code:`outputs` batch represented by the matrix :math:`V_{\\mathrm{out}} \\in \\mathbb{C}^{M \\\\times N}`\n",
    "        Returns:\n",
    "            Inverse propaged :code:`outputs` through single layer :math:`\\ell` to form an array\n",
    "            :math:`V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`.\n",
    "        \"\"\"\n",
    "        inputs = outputs if self.right_perm is None else self.right_perm.inverse_transform(outputs)\n",
    "        diag = tf.math.conj(self.diag)\n",
    "        off_diag = tf.gather(tf.math.conj(self.off_diag), self.pairwise_perm_idx, axis=-1)\n",
    "        inputs = inputs * diag + tf.gather(inputs * off_diag, self.pairwise_perm_idx, axis=-1)\n",
    "        return inputs if self.left_perm is None else self.left_perm.inverse_transform(inputs)\n",
    "\n",
    "\n",
    "class MeshParamTensorflow:\n",
    "    \"\"\"A class that cleanly arranges parameters into a specific arrangement that can be used to simulate any mesh\n",
    "    Args:\n",
    "        param: parameter to arrange in mesh\n",
    "        units: number of inputs/outputs of the mesh\n",
    "    \"\"\"\n",
    "    def __init__(self, param: tf.Tensor, units: int):\n",
    "        self.param = param\n",
    "        self.units = units\n",
    "\n",
    "    @property\n",
    "    def single_mode_arrangement(self):\n",
    "        \"\"\"\n",
    "        The single-mode arrangement based on the :math:`L(\\\\theta)` transfer matrix for :code:`PhaseShiftUpper`\n",
    "        is one where elements of `param` are on the even rows and all odd rows are zero.\n",
    "        In particular, given the :code:`param` array\n",
    "        :math:`\\\\boldsymbol{\\\\theta} = [\\\\boldsymbol{\\\\theta}_1, \\\\boldsymbol{\\\\theta}_2, \\ldots \\\\boldsymbol{\\\\theta}_M]^T`,\n",
    "        where :math:`\\\\boldsymbol{\\\\theta}_m` represent row vectors and :math:`M = \\\\lfloor\\\\frac{N}{2}\\\\rfloor`, the single-mode arrangement has the stripe array form\n",
    "        :math:`\\widetilde{\\\\boldsymbol{\\\\theta}} = [\\\\boldsymbol{\\\\theta}_1, \\\\boldsymbol{0}, \\\\boldsymbol{\\\\theta}_2, \\\\boldsymbol{0}, \\ldots \\\\boldsymbol{\\\\theta}_N, \\\\boldsymbol{0}]^T`.\n",
    "        where :math:`\\widetilde{\\\\boldsymbol{\\\\theta}} \\in \\mathbb{R}^{N \\\\times L}` defines the :math:`\\\\boldsymbol{\\\\theta}` of the final mesh\n",
    "        and :math:`\\\\boldsymbol{0}` represents an array of zeros of the same size as :math:`\\\\boldsymbol{\\\\theta}_n`.\n",
    "        Returns:\n",
    "            Single-mode arrangement array of phases\n",
    "        \"\"\"\n",
    "        tensor_t = tf.transpose(self.param)\n",
    "        stripe_tensor = tf.reshape(tf.concat((tensor_t, tf.zeros_like(tensor_t)), 1),\n",
    "                                   shape=(tensor_t.shape[0] * 2, tensor_t.shape[1]))\n",
    "        if self.units % 2:\n",
    "            return tf.concat([stripe_tensor, tf.zeros(shape=(1, tensor_t.shape[1]))], axis=0)\n",
    "        else:\n",
    "            return stripe_tensor\n",
    "\n",
    "    @property\n",
    "    def common_mode_arrangement(self) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        The common-mode arrangement based on the :math:`C(\\\\theta)` transfer matrix for :code:`PhaseShiftCommonMode`\n",
    "        is one where elements of `param` are on the even rows and repeated on respective odd rows.\n",
    "        In particular, given the :code:`param` array\n",
    "        :math:`\\\\boldsymbol{\\\\theta} = [\\\\boldsymbol{\\\\theta}_1, \\\\boldsymbol{\\\\theta}_2, \\ldots \\\\boldsymbol{\\\\theta}_M]^T`,\n",
    "        where :math:`\\\\boldsymbol{\\\\theta}_n` represent row vectors and :math:`M = \\\\lfloor\\\\frac{N}{2}\\\\rfloor`,\n",
    "        the common-mode arrangement has the stripe array form\n",
    "        :math:`\\\\widetilde{\\\\boldsymbol{\\\\theta}} = [\\\\boldsymbol{\\\\theta}_1, \\\\boldsymbol{\\\\theta}_1,\\\\boldsymbol{\\\\theta}_2, \\\\boldsymbol{\\\\theta}_2, \\ldots \\\\boldsymbol{\\\\theta}_N, \\\\boldsymbol{\\\\theta}_N]^T`.\n",
    "        where :math:`\\widetilde{\\\\boldsymbol{\\\\theta}} \\in \\mathbb{R}^{N \\\\times L}` defines the :math:`\\\\boldsymbol{\\\\theta}` of the final mesh.\n",
    "        Returns:\n",
    "            Common-mode arrangement array of phases\n",
    "        \"\"\"\n",
    "        phases = self.single_mode_arrangement\n",
    "        return phases + _roll_tensor(phases)\n",
    "\n",
    "    @property\n",
    "    def differential_mode_arrangement(self) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        The differential-mode arrangement is based on the :math:`D(\\\\theta)` transfer matrix\n",
    "        for :code:`PhaseShiftDifferentialMode`.\n",
    "        Given the :code:`param` array\n",
    "        :math:`\\\\boldsymbol{\\\\theta} = [\\cdots \\\\boldsymbol{\\\\theta}_m \\cdots]^T`,\n",
    "        where :math:`\\\\boldsymbol{\\\\theta}_n` represent row vectors and :math:`M = \\\\lfloor\\\\frac{N}{2}\\\\rfloor`, the differential-mode arrangement has the form\n",
    "        :math:`\\\\widetilde{\\\\boldsymbol{\\\\theta}} = \\\\left[\\cdots \\\\frac{\\\\boldsymbol{\\\\theta}_m}{2}, -\\\\frac{\\\\boldsymbol{\\\\theta}_m}{2} \\cdots \\\\right]^T`.\n",
    "        where :math:`\\widetilde{\\\\boldsymbol{\\\\theta}} \\in \\mathbb{R}^{N \\\\times L}` defines the :math:`\\\\boldsymbol{\\\\theta}` of the final mesh.\n",
    "        Returns:\n",
    "            Differential-mode arrangement array of phases\n",
    "        \"\"\"\n",
    "        phases = self.single_mode_arrangement\n",
    "        return phases / 2 - _roll_tensor(phases / 2)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return MeshParamTensorflow(self.param + other.param, self.units)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return MeshParamTensorflow(self.param - other.param, self.units)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return MeshParamTensorflow(self.param * other.param, self.units)\n",
    "\n",
    "\n",
    "class MeshPhasesTensorflow:\n",
    "    \"\"\"Organizes the phases in the mesh into appropriate arrangements\n",
    "    Args:\n",
    "        theta: Array to be converted to :math:`\\\\boldsymbol{\\\\theta}`\n",
    "        phi: Array to be converted to :math:`\\\\boldsymbol{\\\\phi}`\n",
    "        gamma: Array to be converted to :math:`\\\\boldsymbol{\\gamma}`\n",
    "        mask: Mask over values of :code:`theta` and :code:`phi` that are not in bar state\n",
    "        basis: Phase basis to use\n",
    "        hadamard: Whether to use Hadamard convention\n",
    "        theta_fn: TF-friendly phi function call to reparametrize phi (example use cases: see `neurophox.helpers`).\n",
    "                By default, use identity function.\n",
    "        phi_fn: TF-friendly phi function call to reparametrize phi (example use cases: see `neurophox.helpers`).\n",
    "                By default, use identity function.\n",
    "        gamma_fn: TF-friendly gamma function call to reparametrize gamma (example use cases: see `neurophox.helpers`).\n",
    "        phase_loss_fn: Incorporate phase shift-dependent loss into the model.\n",
    "                        The function is of the form phase_loss_fn(phases),\n",
    "                        which returns the loss\n",
    "    \"\"\"\n",
    "    def __init__(self, theta: tf.Variable, phi: tf.Variable, mask: np.ndarray, gamma: tf.Variable, units: int,\n",
    "                 basis: str = SINGLEMODE, hadamard: bool = False,\n",
    "                 theta_fn: Optional[Callable] = None, phi_fn: Optional[Callable] = None,\n",
    "                 gamma_fn: Optional[Callable] = None,\n",
    "                 phase_loss_fn: Optional[Callable[[tf.Tensor], tf.Tensor]] = None):\n",
    "        self.mask = mask if mask is not None else np.ones_like(theta)\n",
    "        self.theta_fn = (lambda x: x) if theta_fn is None else theta_fn\n",
    "        self.phi_fn = (lambda x: x) if phi_fn is None else phi_fn\n",
    "        self.gamma_fn = (lambda x: x) if gamma_fn is None else gamma_fn\n",
    "        self.theta = MeshParamTensorflow(self.theta_fn(theta) * mask + (1 - mask) * (1 - hadamard) * np.pi, units=units)\n",
    "        self.phi = MeshParamTensorflow(self.phi_fn(phi) * mask + (1 - mask) * (1 - hadamard) * np.pi, units=units)\n",
    "        self.gamma = self.gamma_fn(gamma)\n",
    "        self.basis = basis\n",
    "        self.phase_loss_fn = (lambda x: 0) if phase_loss_fn is None else phase_loss_fn\n",
    "        self.phase_fn = lambda phase: tf.complex(tf.cos(phase), tf.sin(phase)) * (1 - _to_complex(self.phase_loss_fn(phase)))\n",
    "        self.input_phase_shift_layer = self.phase_fn(self.gamma)\n",
    "        if self.theta.param.shape != self.phi.param.shape:\n",
    "            raise ValueError(\"Internal phases (theta) and external phases (phi) need to have the same shape.\")\n",
    "\n",
    "    @property\n",
    "    def internal_phase_shifts(self):\n",
    "        \"\"\"\n",
    "        The internal phase shift matrix of the mesh corresponds to an `L \\\\times N` array of phase shifts\n",
    "        (in between beamsplitters, thus internal) where :math:`L` is number of layers and :math:`N` is number of inputs/outputs\n",
    "        Returns:\n",
    "            Internal phase shift matrix corresponding to :math:`\\\\boldsymbol{\\\\theta}`\n",
    "        \"\"\"\n",
    "        if self.basis == BLOCH:\n",
    "            return self.theta.differential_mode_arrangement\n",
    "        elif self.basis == SINGLEMODE:\n",
    "            return self.theta.single_mode_arrangement\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.basis} is not yet supported or invalid.\")\n",
    "\n",
    "    @property\n",
    "    def external_phase_shifts(self):\n",
    "        \"\"\"The external phase shift matrix of the mesh corresponds to an `L \\\\times N` array of phase shifts\n",
    "        (outside of beamsplitters, thus external) where :math:`L` is number of layers and :math:`N` is number of inputs/outputs\n",
    "        Returns:\n",
    "            External phase shift matrix corresponding to :math:`\\\\boldsymbol{\\\\phi}`\n",
    "        \"\"\"\n",
    "        if self.basis == BLOCH or self.basis == SINGLEMODE:\n",
    "            return self.phi.single_mode_arrangement\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.basis} is not yet supported or invalid.\")\n",
    "\n",
    "    @property\n",
    "    def internal_phase_shift_layers(self):\n",
    "        \"\"\"Elementwise applying complex exponential to :code:`internal_phase_shifts`.\n",
    "        Returns:\n",
    "            Internal phase shift layers corresponding to :math:`\\\\boldsymbol{\\\\theta}`\n",
    "        \"\"\"\n",
    "        return self.phase_fn(self.internal_phase_shifts)\n",
    "\n",
    "    @property\n",
    "    def external_phase_shift_layers(self):\n",
    "        \"\"\"Elementwise applying complex exponential to :code:`external_phase_shifts`.\n",
    "        Returns:\n",
    "            External phase shift layers corresponding to :math:`\\\\boldsymbol{\\\\phi}`\n",
    "        \"\"\"\n",
    "        return self.phase_fn(self.external_phase_shifts)\n",
    "\n",
    "\n",
    "class Mesh:\n",
    "    def __init__(self, model: MeshModel):\n",
    "        \"\"\"General mesh network layer defined by `neurophox.meshmodel.MeshModel`\n",
    "        Args:\n",
    "            model: The `MeshModel` model of the mesh network (e.g., rectangular, triangular, custom, etc.)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.units, self.num_layers = self.model.units, self.model.num_layers\n",
    "        self.pairwise_perm_idx = pairwise_off_diag_permutation(self.units)\n",
    "        ss, cs, sc, cc = self.model.mzi_error_tensors\n",
    "        self.ss, self.cs, self.sc, self.cc = tf.constant(ss, dtype=TF_COMPLEX), tf.constant(cs, dtype=TF_COMPLEX), \\\n",
    "                                             tf.constant(sc, dtype=TF_COMPLEX), tf.constant(cc, dtype=TF_COMPLEX)\n",
    "        self.perm_layers = [PermutationLayer(self.model.perm_idx[layer]) for layer in range(self.num_layers + 1)]\n",
    "\n",
    "    def mesh_layers(self, phases: MeshPhasesTensorflow) -> List[MeshVerticalLayer]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            phases:  The :code:`MeshPhasesTensorflow` object containing :math:`\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\phi}, \\\\boldsymbol{\\\\gamma}`\n",
    "        Returns:\n",
    "            List of mesh layers to be used by any instance of :code:`MeshLayer`\n",
    "        \"\"\"\n",
    "        internal_psl = phases.internal_phase_shift_layers\n",
    "        external_psl = phases.external_phase_shift_layers\n",
    "        # smooth trick to efficiently perform the layerwise coupling computation\n",
    "\n",
    "        if self.model.hadamard:\n",
    "            s11 = self.cc * internal_psl + self.ss * _roll_tensor(internal_psl, up=True)\n",
    "            s22 = _roll_tensor(self.ss * internal_psl + self.cc * _roll_tensor(internal_psl, up=True))\n",
    "            s12 = _roll_tensor(self.cs * internal_psl - self.sc * _roll_tensor(internal_psl, up=True))\n",
    "            s21 = self.sc * internal_psl - self.cs * _roll_tensor(internal_psl, up=True)\n",
    "        else:\n",
    "            s11 = self.cc * internal_psl - self.ss * _roll_tensor(internal_psl, up=True)\n",
    "            s22 = _roll_tensor(-self.ss * internal_psl + self.cc * _roll_tensor(internal_psl, up=True))\n",
    "            s12 = 1j * _roll_tensor(self.cs * internal_psl + self.sc * _roll_tensor(internal_psl, up=True))\n",
    "            s21 = 1j * (self.sc * internal_psl + self.cs * _roll_tensor(internal_psl, up=True))\n",
    "\n",
    "        diag_layers = external_psl * (s11 + s22) / 2\n",
    "        off_diag_layers = _roll_tensor(external_psl) * (s21 + s12) / 2\n",
    "\n",
    "        if self.units % 2:\n",
    "            diag_layers = tf.concat((diag_layers[:-1], tf.ones_like(diag_layers[-1:])), axis=0)\n",
    "\n",
    "        diag_layers, off_diag_layers = tf.transpose(diag_layers), tf.transpose(off_diag_layers)\n",
    "\n",
    "        mesh_layers = [MeshVerticalLayer(self.pairwise_perm_idx, diag_layers[0], off_diag_layers[0],\n",
    "                                         self.perm_layers[1], self.perm_layers[0])]\n",
    "        for layer in range(1, self.num_layers):\n",
    "            mesh_layers.append(MeshVerticalLayer(self.pairwise_perm_idx, diag_layers[layer], off_diag_layers[layer],\n",
    "                                                 self.perm_layers[layer + 1]))\n",
    "        return mesh_layers\n",
    "\n",
    "\n",
    "class MeshLayer(TransformerLayer):\n",
    "    \"\"\"Mesh network layer for unitary operators implemented in numpy\n",
    "    Args:\n",
    "        mesh_model: The `MeshModel` model of the mesh network (e.g., rectangular, triangular, custom, etc.)\n",
    "        activation: Nonlinear activation function (:code:`None` if there's no nonlinearity)\n",
    "        incoherent: Use an incoherent representation for the layer (no phase coherent between respective inputs...)\n",
    "        phases: Initialize with phases (overrides mesh model initialization)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mesh_model: MeshModel, activation: Activation = None,\n",
    "                 incoherent: bool = False,\n",
    "                 phase_loss_fn: Optional[Callable[[tf.Tensor], tf.Tensor]] = None,\n",
    "                 **kwargs):\n",
    "        self.mesh = Mesh(mesh_model)\n",
    "        self.units, self.num_layers = self.mesh.units, self.mesh.num_layers\n",
    "        self.incoherent = incoherent\n",
    "        self.phase_loss_fn = phase_loss_fn\n",
    "        super(MeshLayer, self).__init__(self.units, activation=activation, **kwargs)\n",
    "        theta_init, phi_init, gamma_init = self.mesh.model.init\n",
    "        self.theta, self.phi, self.gamma = theta_init.to_tf(\"theta\"), phi_init.to_tf(\"phi\"), gamma_init.to_tf(\"gamma\")\n",
    "        self.theta_fn, self.phi_fn, self.gamma_fn = self.mesh.model.theta_fn, self.mesh.model.phi_fn, self.mesh.model.gamma_fn\n",
    "\n",
    "    @tf.function\n",
    "    def transform(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the operation (where :math:`U` represents the matrix for this layer):\n",
    "        .. math::\n",
    "            V_{\\mathrm{out}} = V_{\\mathrm{in}} U,\n",
    "        where :math:`U \\in \\mathrm{U}(N)` and :math:`V_{\\mathrm{out}}, V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`.\n",
    "        Args:\n",
    "            inputs: :code:`inputs` batch represented by the matrix :math:`V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`\n",
    "        Returns:\n",
    "            Transformed :code:`inputs`, :math:`V_{\\mathrm{out}}`\n",
    "        \"\"\"\n",
    "        _inputs = np.eye(self.units, dtype=np.complex64) if self.incoherent else inputs\n",
    "        mesh_phases, mesh_layers = self.phases_and_layers\n",
    "        outputs = _inputs * mesh_phases.input_phase_shift_layer\n",
    "        for layer in range(self.num_layers):\n",
    "            outputs = mesh_layers[layer].transform(outputs)\n",
    "        if self.incoherent:\n",
    "            power_matrix = tf.math.real(outputs) ** 2 + tf.math.imag(outputs) ** 2\n",
    "            power_inputs = tf.math.real(inputs) ** 2 + tf.math.imag(inputs) ** 2\n",
    "            outputs = power_inputs @ power_matrix\n",
    "            return tf.complex(tf.sqrt(outputs), tf.zeros_like(outputs))\n",
    "        return outputs\n",
    "\n",
    "    @tf.function\n",
    "    def inverse_transform(self, outputs: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the operation (where :math:`U` represents the matrix for this layer):\n",
    "        .. math::\n",
    "            V_{\\mathrm{in}} = V_{\\mathrm{out}} U^\\dagger,\n",
    "        where :math:`U \\in \\mathrm{U}(N)` and :math:`V_{\\mathrm{out}}, V_{\\mathrm{in}} \\in \\mathbb{C}^{M \\\\times N}`.\n",
    "        Args:\n",
    "            outputs: :code:`outputs` batch represented by the matrix :math:`V_{\\mathrm{out}} \\in \\mathbb{C}^{M \\\\times N}`\n",
    "        Returns:\n",
    "            Inverse transformed :code:`outputs`, :math:`V_{\\mathrm{in}}`\n",
    "        \"\"\"\n",
    "        mesh_phases, mesh_layers = self.phases_and_layers\n",
    "        inputs = outputs\n",
    "        for layer in reversed(range(self.num_layers)):\n",
    "            inputs = mesh_layers[layer].inverse_transform(inputs)\n",
    "        inputs = inputs * tf.math.conj(mesh_phases.input_phase_shift_layer)\n",
    "        return inputs\n",
    "\n",
    "    @property\n",
    "    def phases_and_layers(self) -> Tuple[MeshPhasesTensorflow, List[MeshVerticalLayer]]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Phases and layers for this mesh layer\n",
    "        \"\"\"\n",
    "        mesh_phases = MeshPhasesTensorflow(\n",
    "            theta=self.theta, phi=self.phi, gamma=self.gamma,\n",
    "            theta_fn=self.theta_fn, phi_fn=self.phi_fn, gamma_fn=self.gamma_fn,\n",
    "            mask=self.mesh.model.mask, hadamard=self.mesh.model.hadamard,\n",
    "            units=self.units, basis=self.mesh.model.basis, phase_loss_fn=self.phase_loss_fn,\n",
    "        )\n",
    "        mesh_layers = self.mesh.mesh_layers(mesh_phases)\n",
    "        return mesh_phases, mesh_layers\n",
    "\n",
    "    @property\n",
    "    def phases(self) -> MeshPhases:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            The :code:`MeshPhases` object for this layer\n",
    "        \"\"\"\n",
    "        return MeshPhases(\n",
    "            theta=self.theta.numpy() * self.mesh.model.mask,\n",
    "            phi=self.phi.numpy() * self.mesh.model.mask,\n",
    "            mask=self.mesh.model.mask,\n",
    "            gamma=self.gamma.numpy()\n",
    "        )\n",
    "\n",
    "\n",
    "def _roll_tensor(tensor: tf.Tensor, up=False):\n",
    "    # a complex number-friendly roll that works on gpu\n",
    "    if up:\n",
    "        return tf.concat([tensor[1:], tensor[tf.newaxis, 0]], axis=0)\n",
    "    return tf.concat([tensor[tf.newaxis, -1], tensor[:-1]], axis=0)\n",
    "\n",
    "\n",
    "def _to_complex(tensor: tf.Tensor):\n",
    "    if isinstance(tensor, tf.Tensor) and tensor.dtype == tf.float32:\n",
    "        return tf.complex(tensor, tf.zeros_like(tensor))\n",
    "    else:\n",
    "        return tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
